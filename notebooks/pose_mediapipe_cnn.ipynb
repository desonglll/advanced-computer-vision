{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-15T08:35:48.145160Z",
     "start_time": "2024-11-15T08:35:48.142592Z"
    }
   },
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, MaxPooling1D\n",
    "\n",
    "from mediapipe_impl.hand_track.HandTrackingMin import success\n"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:23:20.091113Z",
     "start_time": "2024-11-15T13:23:20.082570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in data:\n",
    "        features = sample[\"features\"]\n",
    "        # Flatten each (x, y, z, visibility) into a single array\n",
    "        flattened_features = []\n",
    "        for keypoint in features:\n",
    "            flattened_features.extend([keypoint[\"x\"], keypoint[\"y\"], keypoint[\"z\"], keypoint[\"visibility\"]])\n",
    "\n",
    "        inputs.append(flattened_features)\n",
    "\n",
    "        # Multi-label processing: Convert label list to a binary vector\n",
    "        label_list = sample[\"label\"]\n",
    "        labels.append(label_list)\n",
    "\n",
    "    return np.array(inputs), labels"
   ],
   "id": "f3145b14e537b7e3",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:29:31.617654Z",
     "start_time": "2024-11-15T13:29:31.607540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "X, y = load_data('../data.json')\n",
    "print(f\"Initial X shape: {X.shape}\")\n",
    "ALL_LABELS = []\n",
    "for label in y:\n",
    "    if label not in ALL_LABELS:\n",
    "        ALL_LABELS.append(label)\n",
    "print(f\"ALL LABELS: {ALL_LABELS}\")\n",
    "# N categories\n",
    "N = len(ALL_LABELS)\n",
    "print(f\"N: {N}\")\n",
    "KEY_POINTS = 33\n",
    "FEATURES = 4\n",
    "# Reshape data to fit Conv1D input: (samples, steps, features)\n",
    "X = X.reshape((X.shape[0], KEY_POINTS, 4))  # 33 keypoints with 4 features (x, y, z, visibility)\n",
    "# Checking the shape of the reshaped data\n",
    "print(X.shape)"
   ],
   "id": "f3b415bed9709668",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial X shape: (2, 132)\n",
      "ALL LABELS: ['world', 'hello']\n",
      "N: 2\n",
      "(2, 33, 4)\n"
     ]
    }
   ],
   "execution_count": 87
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:30:50.754591Z",
     "start_time": "2024-11-15T13:30:50.739238Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize a LabelEncoder to convert strings to integers\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the labels to integers\n",
    "y_int = label_encoder.fit_transform(y)\n",
    "# Now apply to_categorical for one-hot encoding\n",
    "y_onehot = to_categorical(y_int, num_classes=len(label_encoder.classes_))\n",
    "print(f\"One-hot encoded labels shape: {y_onehot.shape}\")\n",
    "print(y_onehot)\n"
   ],
   "id": "6aa37bdd4e1bc198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded labels shape: (2, 2)\n",
      "[[0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:31:40.940892Z",
     "start_time": "2024-11-15T13:31:40.885866Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# Add Conv1D layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(KEY_POINTS, FEATURES)))\n",
    "# Add MaxPooling1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Add another Conv1D layer\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "# Add another MaxPooling1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Flatten the output from Conv1D layers\n",
    "model.add(Flatten())\n",
    "# Add Dense layer with dropout for regularization\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Output layer with softmax activation (for classification)\n",
    "model.add(Dense(N, activation='softmax'))  # N is the number of classes"
   ],
   "id": "eabe977a1ebf87b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeshinoda/Library/Caches/pypoetry/virtualenvs/advanced-computer-vision-mRP4g53R-py3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:31:45.039970Z",
     "start_time": "2024-11-15T13:31:45.018530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Summary of the model\n",
    "model.summary()"
   ],
   "id": "de6af3fb27d91125",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_7\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_14 (\u001B[38;5;33mConv1D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m31\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │           \u001B[38;5;34m832\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (\u001B[38;5;33mMaxPooling1D\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_15 (\u001B[38;5;33mConv1D\u001B[0m)              │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │        \u001B[38;5;34m24,704\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_15 (\u001B[38;5;33mMaxPooling1D\u001B[0m) │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m, \u001B[38;5;34m128\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m768\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m98,432\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (\u001B[38;5;33mDense\u001B[0m)                │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2\u001B[0m)              │           \u001B[38;5;34m258\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m124,226\u001B[0m (485.26 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,226</span> (485.26 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m124,226\u001B[0m (485.26 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,226</span> (485.26 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:17:07.347429Z",
     "start_time": "2024-11-15T13:17:06.957112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the Model\n",
    "history = model.fit(X, y_onehot, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X, y_onehot)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ],
   "id": "c39b501a12ba927a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 42ms/step - accuracy: 1.0000 - loss: 0.0286 - val_accuracy: 0.0000e+00 - val_loss: 4.4675\n",
      "Epoch 2/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step - accuracy: 1.0000 - loss: 0.0158 - val_accuracy: 0.0000e+00 - val_loss: 4.9717\n",
      "Epoch 3/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step - accuracy: 1.0000 - loss: 0.0144 - val_accuracy: 0.0000e+00 - val_loss: 5.4782\n",
      "Epoch 4/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 33ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 0.0000e+00 - val_loss: 5.9735\n",
      "Epoch 5/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 28ms/step - accuracy: 1.0000 - loss: 0.0031 - val_accuracy: 0.0000e+00 - val_loss: 6.4554\n",
      "Epoch 6/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step - accuracy: 1.0000 - loss: 3.2765e-04 - val_accuracy: 0.0000e+00 - val_loss: 6.9152\n",
      "Epoch 7/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 28ms/step - accuracy: 1.0000 - loss: 0.0053 - val_accuracy: 0.0000e+00 - val_loss: 7.3631\n",
      "Epoch 8/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 31ms/step - accuracy: 1.0000 - loss: 0.0010 - val_accuracy: 0.0000e+00 - val_loss: 7.7902\n",
      "Epoch 9/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 27ms/step - accuracy: 1.0000 - loss: 3.4994e-04 - val_accuracy: 0.0000e+00 - val_loss: 8.2077\n",
      "Epoch 10/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 1.0000 - loss: 5.8053e-05 - val_accuracy: 0.0000e+00 - val_loss: 8.6008\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - accuracy: 0.5000 - loss: 4.3005\n",
      "Accuracy: 50.00%\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:17:21.335248Z",
     "start_time": "2024-11-15T13:17:21.309100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from mediapipe_impl.pose_estimation import PoseEstimationModule as pm\n",
    "\n",
    "detector = pm.PoseDetector()\n",
    "\n",
    "\n",
    "def extract_keypoints(image_path):\n",
    "    cap = cv2.VideoCapture(image_path)\n",
    "    success, img = cap.read()\n",
    "    img = detector.find_pose(img=img)\n",
    "    lm_list = detector.find_position(img, draw=False)\n",
    "    return lm_list"
   ],
   "id": "7918daa27179925b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731676641.330146 4763801 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 83.1), renderer: Apple M1\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T13:26:43.002912Z",
     "start_time": "2024-11-15T13:26:42.897568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_image(image_path):\n",
    "    data = []\n",
    "    keypoints = extract_keypoints(image_path)\n",
    "    for keypoint in keypoints:\n",
    "        data.extend([keypoint[\"x\"], keypoint[\"y\"], keypoint[\"z\"], keypoint[\"visibility\"]])\n",
    "    X = np.array(data).reshape((1, 33, 4))\n",
    "    print(X.shape)\n",
    "    # 模型预测\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # 获取预测类别的索引\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(ALL_LABELS[predicted_class])\n",
    "\n",
    "\n",
    "# 示例：预测一张新图像\n",
    "image_path = '../datasets/img.jpg'\n",
    "result = predict_image(image_path)"
   ],
   "id": "a9b9c16f7d945f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 33, 4)\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 10ms/step\n",
      "Predicted Class: 1\n",
      "hello\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b843624e1e98a5dc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
