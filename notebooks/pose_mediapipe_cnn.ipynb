{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Extract Part",
   "id": "30fe77353875170c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:20.859598Z",
     "start_time": "2024-11-16T12:16:20.840911Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "from mediapipe_impl.pose_estimation import PoseEstimationModule as pm\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "detector = pm.PoseDetector()\n",
    "\n",
    "images_dir = \"../datasets/img\"\n",
    "images = [os.path.join(images_dir, img) for img in os.listdir(images_dir) if img.endswith(('.png', '.jpg', '.jpeg'))]"
   ],
   "id": "6c4cb1c54556d64b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731759380.854248 5614427 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 83.1), renderer: Apple M1\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.314920Z",
     "start_time": "2024-11-16T12:16:20.880072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for image_path in images:\n",
    "    print(image_path)\n",
    "    # success, img = cap.read()\n",
    "    # img = cv2.resize(img, (1280, 720))\n",
    "    img = cv2.imread(image_path)\n",
    "    img = detector.find_pose(img, draw=False)\n",
    "    lm_list = detector.find_position(img, False)\n",
    "    cv2.imshow('img', img)\n",
    "    cv2.waitKey(1)  # 等待1毫秒刷新窗口\n",
    "\n",
    "    # label = input(f\"Please enter the label (0/1): \")\n",
    "    data = {\n",
    "        \"filename\": image_path.split('/')[-1],\n",
    "        \"features\": lm_list,\n",
    "    }\n",
    "\n",
    "    # 读取现有JSON文件（如果存在），将新数据追加到列表中\n",
    "    json_file_path = \"../data.json\"\n",
    "    try:\n",
    "        with open(json_file_path, \"r\") as file:\n",
    "            data_list = json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        data_list = []\n",
    "\n",
    "    # 追加新数据\n",
    "    data_list.append(data)\n",
    "\n",
    "    # 将更新后的列表写回JSON文件\n",
    "    with open(json_file_path, \"w\") as file:\n",
    "        json.dump(data_list, file, indent=4)\n",
    "\n",
    "    # 关闭窗口并退出循环\n",
    "    cv2.destroyAllWindows()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "2fadc7c68e69e96a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1731759380.921806 5676431 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1731759380.938731 5676431 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../datasets/img/img_10.jpg\n",
      "../datasets/img/img_38.jpg\n",
      "../datasets/img/img_1.jpg\n",
      "../datasets/img/img_39.jpg\n",
      "../datasets/img/img_11.jpg\n",
      "../datasets/img/img_13.jpg\n",
      "../datasets/img/img_100.jpg\n",
      "../datasets/img/img_2.jpg\n",
      "../datasets/img/img_3.jpg\n",
      "../datasets/img/img_12.jpg\n",
      "../datasets/img/img_16.jpg\n",
      "../datasets/img/img_7.jpg\n",
      "../datasets/img/img_6.jpg\n",
      "../datasets/img/img_17.jpg\n",
      "../datasets/img/img_29.jpg\n",
      "../datasets/img/img_15.jpg\n",
      "../datasets/img/img_4.jpg\n",
      "../datasets/img/img_5.jpg\n",
      "../datasets/img/img_14.jpg\n",
      "../datasets/img/img_28.jpg\n",
      "../datasets/img/img_98.jpg\n",
      "../datasets/img/img_73.jpg\n",
      "../datasets/img/img_67.jpg\n",
      "../datasets/img/img_66.jpg\n",
      "../datasets/img/img_72.jpg\n",
      "../datasets/img/img_99.jpg\n",
      "../datasets/img/img_64.jpg\n",
      "../datasets/img/img_70.jpg\n",
      "../datasets/img/img_58.jpg\n",
      "../datasets/img/img_59.jpg\n",
      "../datasets/img/img_71.jpg\n",
      "../datasets/img/img_65.jpg\n",
      "../datasets/img/img_49.jpg\n",
      "../datasets/img/img_61.jpg\n",
      "../datasets/img/img_75.jpg\n",
      "../datasets/img/img_74.jpg\n",
      "../datasets/img/img_60.jpg\n",
      "../datasets/img/img_48.jpg\n",
      "../datasets/img/img_89.jpg\n",
      "../datasets/img/img_76.jpg\n",
      "../datasets/img/img_62.jpg\n",
      "../datasets/img/img_63.jpg\n",
      "../datasets/img/img_77.jpg\n",
      "../datasets/img/img_88.jpg\n",
      "../datasets/img/img_91.jpg\n",
      "../datasets/img/img_85.jpg\n",
      "../datasets/img/img_52.jpg\n",
      "../datasets/img/img_46.jpg\n",
      "../datasets/img/img_47.jpg\n",
      "../datasets/img/img_53.jpg\n",
      "../datasets/img/img_84.jpg\n",
      "../datasets/img/img_90.jpg\n",
      "../datasets/img/img_86.jpg\n",
      "../datasets/img/img_92.jpg\n",
      "../datasets/img/img_45.jpg\n",
      "../datasets/img/img_51.jpg\n",
      "../datasets/img/img_79.jpg\n",
      "../datasets/img/img_78.jpg\n",
      "../datasets/img/img_50.jpg\n",
      "../datasets/img/img_44.jpg\n",
      "../datasets/img/img_93.jpg\n",
      "../datasets/img/img_87.jpg\n",
      "../datasets/img/img_83.jpg\n",
      "../datasets/img/img_97.jpg\n",
      "../datasets/img/img_68.jpg\n",
      "../datasets/img/img_40.jpg\n",
      "../datasets/img/img_54.jpg\n",
      "../datasets/img/img_55.jpg\n",
      "../datasets/img/img_41.jpg\n",
      "../datasets/img/img_69.jpg\n",
      "../datasets/img/img_96.jpg\n",
      "../datasets/img/img_82.jpg\n",
      "../datasets/img/img_94.jpg\n",
      "../datasets/img/img_80.jpg\n",
      "../datasets/img/img_57.jpg\n",
      "../datasets/img/img_43.jpg\n",
      "../datasets/img/img_42.jpg\n",
      "../datasets/img/img_56.jpg\n",
      "../datasets/img/img_81.jpg\n",
      "../datasets/img/img_95.jpg\n",
      "../datasets/img/img_31.jpg\n",
      "../datasets/img/img_25.jpg\n",
      "../datasets/img/img_19.jpg\n",
      "../datasets/img/img_8.jpg\n",
      "../datasets/img/img_9.jpg\n",
      "../datasets/img/img_18.jpg\n",
      "../datasets/img/img_24.jpg\n",
      "../datasets/img/img_30.jpg\n",
      "../datasets/img/img_26.jpg\n",
      "../datasets/img/img_32.jpg\n",
      "../datasets/img/img_33.jpg\n",
      "../datasets/img/img_27.jpg\n",
      "../datasets/img/img_23.jpg\n",
      "../datasets/img/img_37.jpg\n",
      "../datasets/img/img_36.jpg\n",
      "../datasets/img/img_22.jpg\n",
      "../datasets/img/img_34.jpg\n",
      "../datasets/img/img_20.jpg\n",
      "../datasets/img/img_21.jpg\n",
      "../datasets/img/img_35.jpg\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Data Label Part",
   "id": "8313e793ce047f91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Combine Data Part",
   "id": "72bc8863b5e0297d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.326571Z",
     "start_time": "2024-11-16T12:16:30.323570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def jsonl_to_json(jsonl_file_path, output_json_file_path):\n",
    "    \"\"\"\n",
    "    将 JSONL 文件中的数据转换为标准的 JSON 格式文件。\n",
    "\n",
    "    :param jsonl_file_path: JSONL 文件路径\n",
    "    :param output_json_file_path: 输出的 JSON 文件路径\n",
    "    \"\"\"\n",
    "    merged_data = []\n",
    "\n",
    "    # 打开 JSONL 文件并逐行处理\n",
    "    with open(jsonl_file_path, 'r') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            item = json.loads(line)  # 解析每一行的 JSON 对象\n",
    "            merged_data.append(item)\n",
    "\n",
    "    # 将合并后的数据写入输出 JSON 文件\n",
    "    with open(output_json_file_path, 'w') as json_file:\n",
    "        json.dump(merged_data, json_file, indent=4)\n",
    "\n",
    "    print(f\"Converted JSONL data saved to {output_json_file_path}\")\n"
   ],
   "id": "465af7df0a7e1c67",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.336351Z",
     "start_time": "2024-11-16T12:16:30.332980Z"
    }
   },
   "cell_type": "code",
   "source": "jsonl_to_json(\"../all.jsonl\", \"../all.json\")",
   "id": "7f0506e0474a202a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted JSONL data saved to ../all.json\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.415856Z",
     "start_time": "2024-11-16T12:16:30.412974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def merge_features_by_filename(file1_path, file2_path, output_path):\n",
    "    # 读取两个文件的内容\n",
    "    with open(file1_path, 'r') as f1:\n",
    "        list1 = json.load(f1)\n",
    "\n",
    "    with open(file2_path, 'r') as f2:\n",
    "        list2 = json.load(f2)\n",
    "\n",
    "    # 创建一个以 filename 为键的字典，便于快速查找\n",
    "    features_dict = {item[\"filename\"]: item[\"features\"] for item in list1}\n",
    "\n",
    "    # 遍历 list2，将 features 合并\n",
    "    for item in list2:\n",
    "        filename = item[\"filename\"]\n",
    "        if filename in features_dict:\n",
    "            item[\"features\"] = features_dict[filename]\n",
    "\n",
    "    # 将合并后的数据写入输出文件\n",
    "    with open(output_path, 'w') as out_file:\n",
    "        json.dump(list2, out_file, indent=4)\n",
    "\n",
    "    print(f\"Merged data saved to {output_path}\")\n"
   ],
   "id": "8cf4843d474d1ddb",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.468909Z",
     "start_time": "2024-11-16T12:16:30.437988Z"
    }
   },
   "cell_type": "code",
   "source": "merge_features_by_filename(\"../data.json\", \"../all.json\", \"../merged_data.json\")",
   "id": "2e83325e2d22599b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to ../merged_data.json\n"
     ]
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Model Train Part"
   ],
   "id": "1abd031bcb7a4bb0"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.476997Z",
     "start_time": "2024-11-16T12:16:30.475429Z"
    }
   },
   "source": [
    "import json\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:16:30.487925Z",
     "start_time": "2024-11-16T12:16:30.485231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_data(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for sample in data:\n",
    "        if sample[\"label\"]:\n",
    "            features = sample[\"features\"]\n",
    "            # Flatten each (x, y, z, visibility) into a single array\n",
    "            flattened_features = []\n",
    "            for keypoint in features:\n",
    "                flattened_features.extend([keypoint[\"x\"], keypoint[\"y\"], keypoint[\"z\"], keypoint[\"visibility\"]])\n",
    "\n",
    "            inputs.append(flattened_features)\n",
    "\n",
    "            # Multi-label processing: Convert label list to a binary vector\n",
    "            label_list = sample[\"label\"]\n",
    "            labels.append(label_list)\n",
    "\n",
    "    return np.array(inputs), labels"
   ],
   "id": "f3145b14e537b7e3",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:17:20.822610Z",
     "start_time": "2024-11-16T12:17:20.771179Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load data\n",
    "print(\"Load Data\")\n",
    "X, y = load_data('../merged_data.json')\n",
    "print(X)\n",
    "print(y)\n",
    "print(f\"Initial X shape: {X.shape}\")\n",
    "ALL_LABELS = []\n",
    "for label in y:\n",
    "    if label and label not in ALL_LABELS:\n",
    "        ALL_LABELS.append(label)\n",
    "print(f\"ALL LABELS: {ALL_LABELS}\")\n",
    "# N categories\n",
    "N = len(ALL_LABELS)\n",
    "print(f\"N: {N}\")\n",
    "KEY_POINTS = 33\n",
    "FEATURES = 4\n",
    "# Reshape data to fit Conv1D input: (samples, steps, features)\n",
    "X = X.reshape((X.shape[0], KEY_POINTS, 4))  # 33 keypoints with 4 features (x, y, z, visibility)\n",
    "# Checking the shape of the reshaped data\n",
    "print(X.shape)"
   ],
   "id": "f3b415bed9709668",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Data\n",
      "[[ 5.47724485e-01  3.36077452e-01 -1.35032237e+00 ...  2.18307185e+00\n",
      "   7.85210133e-02  1.20109795e-02]\n",
      " [ 5.50327599e-01  3.45828354e-01 -1.20299661e+00 ...  2.22794843e+00\n",
      "  -5.52580468e-02  1.42122142e-03]\n",
      " [ 5.49107134e-01  3.41806620e-01 -1.15832043e+00 ...  2.21742845e+00\n",
      "  -2.66200844e-02  2.13399483e-03]\n",
      " ...\n",
      " [ 5.49295306e-01  3.41853082e-01 -1.22420025e+00 ...  2.20655656e+00\n",
      "   1.32704213e-01  4.39481530e-03]\n",
      " [ 5.49370110e-01  3.40936750e-01 -1.21732807e+00 ...  2.20889902e+00\n",
      "   1.12884142e-01  3.36590293e-03]\n",
      " [ 5.49224079e-01  3.43372256e-01 -1.17433846e+00 ...  2.21863818e+00\n",
      "  -4.61158492e-02  1.99269177e-03]]\n",
      "[['Blur'], ['Normal'], ['Blur'], ['Normal'], ['Blur'], ['Blur'], ['Wrong'], ['Blur'], ['Blur'], ['Wrong'], ['Blur'], ['Wrong'], ['Blur'], ['Wrong'], ['Normal'], ['Normal'], ['Normal'], ['Normal'], ['Normal'], ['Normal']]\n",
      "Initial X shape: (20, 132)\n",
      "ALL LABELS: [['Blur'], ['Normal'], ['Wrong']]\n",
      "N: 3\n",
      "(20, 33, 4)\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:17:55.297264Z",
     "start_time": "2024-11-16T12:17:55.281987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Initialize a LabelEncoder to convert strings to integers\n",
    "label_encoder = LabelEncoder()\n",
    "# Fit and transform the labels to integers\n",
    "y_int = label_encoder.fit_transform(y)\n",
    "# Now apply to_categorical for one-hot encoding\n",
    "y_onehot = to_categorical(y_int, num_classes=len(label_encoder.classes_))\n",
    "print(f\"One-hot encoded labels shape: {y_onehot.shape}\")\n",
    "print(y_onehot)\n"
   ],
   "id": "6aa37bdd4e1bc198",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded labels shape: (20, 3)\n",
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeshinoda/Library/Caches/pypoetry/virtualenvs/advanced-computer-vision-mRP4g53R-py3.12/lib/python3.12/site-packages/sklearn/preprocessing/_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:18:00.305460Z",
     "start_time": "2024-11-16T12:18:00.249518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# Add Conv1D layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(KEY_POINTS, FEATURES)))\n",
    "# Add MaxPooling1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Add another Conv1D layer\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "# Add another MaxPooling1D layer\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "# Flatten the output from Conv1D layers\n",
    "model.add(Flatten())\n",
    "# Add Dense layer with dropout for regularization\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# Output layer with softmax activation (for classification)\n",
    "model.add(Dense(N, activation='softmax'))  # N is the number of classes"
   ],
   "id": "eabe977a1ebf87b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikeshinoda/Library/Caches/pypoetry/virtualenvs/advanced-computer-vision-mRP4g53R-py3.12/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:18:01.348291Z",
     "start_time": "2024-11-16T12:18:01.326510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Summary of the model\n",
    "model.summary()"
   ],
   "id": "de6af3fb27d91125",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential_1\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001B[38;5;33mConv1D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m31\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │           \u001B[38;5;34m832\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001B[38;5;33mMaxPooling1D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m15\u001B[0m, \u001B[38;5;34m64\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001B[38;5;33mConv1D\u001B[0m)               │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m13\u001B[0m, \u001B[38;5;34m128\u001B[0m)        │        \u001B[38;5;34m24,704\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001B[38;5;33mMaxPooling1D\u001B[0m)  │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m6\u001B[0m, \u001B[38;5;34m128\u001B[0m)         │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001B[38;5;33mFlatten\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m768\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │        \u001B[38;5;34m98,432\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001B[38;5;33mDropout\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m128\u001B[0m)            │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m)              │           \u001B[38;5;34m387\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">832</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">387</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m124,355\u001B[0m (485.76 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,355</span> (485.76 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m124,355\u001B[0m (485.76 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">124,355</span> (485.76 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:18:03.114994Z",
     "start_time": "2024-11-16T12:18:02.188529Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the Model\n",
    "history = model.fit(X, y_onehot, epochs=10, batch_size=32, validation_split=0.2)\n",
    "# Evaluate the Model\n",
    "loss, accuracy = model.evaluate(X, y_onehot)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ],
   "id": "c39b501a12ba927a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 600ms/step - accuracy: 0.1875 - loss: 1.1439 - val_accuracy: 0.0000e+00 - val_loss: 1.0762\n",
      "Epoch 2/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step - accuracy: 0.5000 - loss: 1.0460 - val_accuracy: 0.0000e+00 - val_loss: 1.1886\n",
      "Epoch 3/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step - accuracy: 0.5000 - loss: 1.0957 - val_accuracy: 0.0000e+00 - val_loss: 1.3054\n",
      "Epoch 4/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 24ms/step - accuracy: 0.5625 - loss: 1.0425 - val_accuracy: 0.0000e+00 - val_loss: 1.4066\n",
      "Epoch 5/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 25ms/step - accuracy: 0.5000 - loss: 1.0257 - val_accuracy: 0.0000e+00 - val_loss: 1.4844\n",
      "Epoch 6/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 23ms/step - accuracy: 0.5000 - loss: 1.0455 - val_accuracy: 0.0000e+00 - val_loss: 1.5463\n",
      "Epoch 7/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 21ms/step - accuracy: 0.5000 - loss: 1.0117 - val_accuracy: 0.0000e+00 - val_loss: 1.5920\n",
      "Epoch 8/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 22ms/step - accuracy: 0.5000 - loss: 0.9687 - val_accuracy: 0.0000e+00 - val_loss: 1.6236\n",
      "Epoch 9/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step - accuracy: 0.5000 - loss: 0.9754 - val_accuracy: 0.0000e+00 - val_loss: 1.6605\n",
      "Epoch 10/10\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 26ms/step - accuracy: 0.5000 - loss: 0.9345 - val_accuracy: 0.0000e+00 - val_loss: 1.6931\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - accuracy: 0.4000 - loss: 1.1830\n",
      "Accuracy: 40.00%\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:18:04.008255Z",
     "start_time": "2024-11-16T12:18:03.992337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "from mediapipe_impl.pose_estimation import PoseEstimationModule as pm\n",
    "\n",
    "detector = pm.PoseDetector()\n",
    "\n",
    "\n",
    "def extract_keypoints(image_path):\n",
    "    cap = cv2.VideoCapture(image_path)\n",
    "    success, img = cap.read()\n",
    "    img = detector.find_pose(img=img)\n",
    "    lm_list = detector.find_position(img, draw=False)\n",
    "    return lm_list"
   ],
   "id": "7918daa27179925b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731759484.003967 5614427 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 83.1), renderer: Apple M1\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T12:18:05.376942Z",
     "start_time": "2024-11-16T12:18:05.251337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict_image(image_path):\n",
    "    data = []\n",
    "    keypoints = extract_keypoints(image_path)\n",
    "    for keypoint in keypoints:\n",
    "        data.extend([keypoint[\"x\"], keypoint[\"y\"], keypoint[\"z\"], keypoint[\"visibility\"]])\n",
    "    X = np.array(data).reshape((1, 33, 4))\n",
    "    print(X.shape)\n",
    "    # 模型预测\n",
    "    predictions = model.predict(X)\n",
    "\n",
    "    # 获取预测类别的索引\n",
    "    predicted_class = np.argmax(predictions, axis=1)[0]\n",
    "\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "    print(ALL_LABELS[predicted_class])\n",
    "\n",
    "\n",
    "# 示例：预测一张新图像\n",
    "image_path = '../datasets/img.jpg'\n",
    "result = predict_image(image_path)"
   ],
   "id": "a9b9c16f7d945f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 33, 4)\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 29ms/step\n",
      "Predicted Class: 0\n",
      "['Blur']\n"
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "b843624e1e98a5dc",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
